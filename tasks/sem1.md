# Семинар 1

**Статья:** [Understanding Host Network Stack Overheads](https://netdevconf.info/0x15/papers/29/paper.pdf) (SIGCOMM 2021)

## Мини-задание (Lightning-Talk)

Мы привыкли думать, что в 100Гбит/с узкое место, это сеть. Однако замеры на Linux-стеке показал обратное, дело в хосте. Современный стек позволяет получить до 42Гбит/с на одно ядро CPU, причем половина CPU уходит просто на копирование данных из ядра в приложение. В то время как отправить можно со скоростью 89Гбит/с. Увеличение Rx-буфера с помощью DDIO не решает проблему - BDP (объем данных в полете) становиться больше чем размер кеша, из-за чего возрастает количество кэш-мисов. Также критична и локальность в NUMA - перенос на удаленную от NIC NUMA уменьшает пропускную способность на 20%. Таких тонкостей на стеке очень много, встает вопрос, что с этим делать?

Можно целиться на zero-copy на приемнике, конфигурировать Rx-буфер обращая внимание на L3, включить TSO/GRO/Jumbo/aRFS. При настройке мониторить не общую пропускную способность, а потенциал каждого ядра + долю времени, в котором CPU копирует данные. В представленной статье, точные цифры, профили, графики и последствия для ОС и хоста. Если вы строите или планируете работать при нагрузках 100+Гбит/с, это набор практичных ориентиров и узких мест - рекомендуемо к ознакомлению, прочтение, окупится очень быстро
